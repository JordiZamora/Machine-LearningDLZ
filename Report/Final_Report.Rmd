---
title: 'Machine Learning: Kaggle competition on Prediction of a forest covertype'
author: "Samantha Dalton, Yevgen Levin, and Jordi Zamora Munt"
date: "26/03/2015"
output: pdf_document
---

**Goal**

Use a training data set of 53 features and 50000 observations to predict from a testing set of 100000 observations 7 different forest covertypes. Using the Kaggle competition platform we will evaluate the performance of the method by measuring the accuracy on the testing set.

Previous works have reached up to 3\% of error rate with the whole data set that contains more than 500k examples by using random forests (*http://www.wise.io/blog/benchmarking-random-forest-part-1*).

**Introduction of the data**

The data provided is divided in a training set and a testing set. The training set contains 50000 examples with 53 features plus the corresponding id's and covertypes. The structure of the features is as follows:

- Name / Data Type / Measurement / Description
\begin{itemize}
\item id
\item Elevation / quantitative /meters / Elevation in meters
\item Aspect / quantitative / azimuth / Aspect in degrees azimuth
\item Slope / quantitative / degrees / Slope in degrees
\item Horizontal-Distance-To-Hydrology / quantitative / meters / Horz Dist to nearest surface water features
\item Vertical-Distance-To-Hydrology / quantitative / meters / Vert Dist to nearest surface water features
\item Horizontal-Distance-To-Roadways / quantitative / meters / Horz Dist to nearest roadway
\item Hillshade-9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice
\item Hillshade-Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice
\item Hillshade-3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice
\item Horizontal-Distance-To-Fire-Points / quantitative / meters / Horz Dist to nearest wildfire ignition points
\item Wilderness-Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation
\item Soil-Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation
\item Cover-Type (7 types) / integer / 1 to 7 / Forest Cover Type designation
\end{itemize}

The testing data contains the same features except for the Cover-Type that is missed and it is what we want to predict.

**Visual exploration of the training data**

The first data check we should do is look at the distribution of  Cover_Type in the data so we know if we are dealing with equal sample sizes across each cover type.

```{r, echo=FALSE, message=FALSE}
source('multiplot.R')
library(ggplot2)

#Load data
rawdata <- read.csv("../data/Kaggle_Covertype_training.csv")
numcol <- ncol(rawdata)
rawdata[,numcol] <- factor(rawdata[,numcol]) #Classes as factors

table (rawdata[,numcol])
```

The table shows that covertypes 1 and 2 are much more prevalent than other types in the training data. Covertypes 4 and 5 are poorly represented in the training set which can lead to difficulties when we will try to train a model to predict these underrepresented groups.

Another way to understand the data before modeling it is to do some simple visualization. To view variation among covertypes across the continuous variables in the data we created density plots of these variables colored by covertype.

```{r, echo=FALSE, fig.height=20, fig.width=15, message=FALSE}
### Plot the density distribution of the continuous variables 
plotList <- list()

plotList[[1]] <- ggplot(data=rawdata,aes(x=elevation,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[2])+
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[2]] <- ggplot(data=rawdata,aes(x=aspect,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[3])+
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="top")
plotList[[3]] <- ggplot(data=rawdata,aes(x=slope,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[4]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[4]] <- ggplot(data=rawdata,aes(x=hor_dist_hyd,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[5]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[5]] <- ggplot(data=rawdata,aes(x=ver_dist_hyd,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[6]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[6]] <- ggplot(data=rawdata,aes(x=hor_dist_road,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[7]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[7]] <- ggplot(data=rawdata,aes(x=hill_9am,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[8]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[8]] <- ggplot(data=rawdata,aes(x=hill_noon,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[9]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[9]] <- ggplot(data=rawdata,aes(x=hill_3pm,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[10]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
plotList[[10]] <- ggplot(data=rawdata,aes(x=hor_dist_fire,fill=Cover_Type,col=Cover_Type)) + 
  geom_density(alpha = 0.2) +
  xlab(names(rawdata)[11]) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")

layout <- matrix(c(seq(1,10)), nrow = 5, byrow = TRUE)
myPlotList <- plotList
multiplot(plotlist = myPlotList, layout = layout)
```

The features that seem to capture display the most variation among covertypes seems to be captured by elevation and aspect. The rest of the features show a strong overlaping of the covertypes.

For the binary features, we show a table that summarizes the 4 wild areas and the 40 soil types.

```{r, echo=FALSE}
categorical <- rawdata[,12:55]
summaryCategorical <- data.frame(Perc=round(colMeans(categorical),3), counts=round(colSums(categorical),3))
wilds <- apply(categorical[,1:4],2,function(x) x*as.numeric(rawdata[,56]))
wilds2 <- apply(wilds,2, function(x) table(x))

soils <- apply(categorical[,5:ncol(categorical)],2,function(x) x*as.numeric(rawdata[,56]))
soils2 <- apply(soils,2, function(x) table(x))


summaryCategorical <- cbind(summaryCategorical,
                            Cov_1=0,
                            Cov_2=0,
                            Cov_3=0,
                            Cov_4=0,
                            Cov_5=0,
                            Cov_6=0,
                            Cov_7=0)

for (j in 1:length(wilds2)){
  name <- names(wilds2[[j]])[-1]
  for (i in name){
    ii <- as.numeric(i)
    jj <- which(name==i)+1
    summaryCategorical[j,(2+ii)] <- wilds2[[j]][jj]
  }
}


for (j in 1:length(soils2)){
  name <- names(soils2[[j]])[-1]
  for (i in name){
    ii <- as.numeric(i)
    jj <- which(name==i)+1
    summaryCategorical[(length(wilds2)+j),(2+ii)] <- soils2[[j]][jj]
  }
}

#Output the statistics of categorical data
summaryCategorical
```

The first thing we notice is that there are no covertypes with nonzero values for soiltype 15. It means this feature has no predictive power and we should remove it from the feature list. A second important observation is that examples of covers 1 and 2 are both desbriced by similar counts of soils and wild areas. Finally, soiltypes 7 and 37 only contain observations with covers of type 2 and 7 respectively which can be an unique identifier of class type if an observation has one of these soiltypes indicated.

**Proposed methods**

We have done some data preprocessing using different approaches.

\begin{itemize}
\item Raw data inspection: From the previous section we removed soiltype 15 since we do not expect any predictive power from it.
\item Data scaling: We evaluated the accuracy of the different methods by standardizing the data. Three different cases where compared: Scaling all features, Scaling the continuous valued features, weighting the binary features.
\item Dimension reduction methods: Together with the previous scaling we used the R packages `princomp` for principal component analysis and `svd` for singular value decomposition. We wanted to apply the classification algorithms in a lower dimensional space and remove redundant factors.
\end{itemize}

We have focused our classification efforts on three classification methods: Random Forest,  Support vector machines and k-Nearest Neighbours.

This first method we tried for classification was Random Forest. This is a popular ensamble technique that relies on selecting random subsets of the features and random subsamples of the data to partition the features space with axis parallel cuts (trees). It generates a set of partitions each of which is limited during the division process and is labeled by majority vote among the data points that are in that partition. We used the `randomForest` package that implements Breiman's random forest algorithm (see http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm for further details). Because Random Forest is quite popular for classification problems, it seemed appropraite to pursue this method for this problem.


The second approach we attempted for classification was Support Vector Machines. SVMs are used fairly often in supervised learning. Thus they seemed like a viable option for classification. Also since the data did not appear to be linearly separable at first glance, SVMs are appealing because they can efficiently perform nonlinear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.



The final approach we tried and we used in our final submission (with a  more sophisticated implementation using weighted binary feature) was the k-Nearest Neighbors algorithm. kNN is a simple algorithm that calculates distance from a point to its k nearest neighbors.  It assigns the point to the same class as the majority class of its k nearest neighbors.   The class package in R only allows for distance of features to be measured using Euclidian distance, which can be a limitation.

**randomForest**

Raw random forest:
The best results using random forest were obtained by using all the features as provided in the original data. Using the function `tuneRF` we were able to do a coarse grain selection of the number of features randomly chosen on each iteration of the method (*wtry* parameter). The optimal *mtry* was around 40 that is far from the *mtry=7*, that is the default value, due to the sparsity of the binary data. In a second step we perform a more systematic optimization around this value. In this process we cross-validated the accuracy with 8 buckets out-of-sample runing the code in AWS with 8 cores. The results are shown in the figure below.


```{r, warning=FALSE, echo=FALSE}
RFMisclassVsMtryVsNtreeA <- read.csv("Result_Xval_AllFeatures_Systematic.csv")
RFMisclassVsMtryVsNtreeB <- read.csv("Result_Xval_AllFeatures_SystematicB.csv")
RFMisclassVsMtryVsNtree <- rbind(RFMisclassVsMtryVsNtreeA,RFMisclassVsMtryVsNtreeB)
RFMisclassVsMtryVsNtree[,"mtry"] <- factor(RFMisclassVsMtryVsNtree[,"mtry"])
ggplot(data=RFMisclassVsMtryVsNtree, 
       aes(x=ntree,y=(1-Misclass), group=mtry, col=mtry)) + 
  geom_line()+ 
  geom_point() +
  ylab(c("Accuracy")) +
  xlim(c(0,1000)) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14))
```

The accuracy grow assimptotically to $\sim 89.5$ for all the *mtrys*. The optimum accuracy of 0.8953 is obtained for *mtry=41* and *ntree=1000*. However, in Kaggle we decided to submit our second best guess that was for *mtry=39* and *ntree=1000* and it returned a decent 0.90090 of accuracy.

We also tryed other approaches that are summarized in the following table:

\begin{tabular}{|l|l||l|l||l|l|}\hline
Method & Preprocessing & Accuracy(\%) & mtry & ntree & Other Params\\ \hline
Random Forest & raw data &  89.47 & 39 & 1000 & \\
Random Forest & PCA without scaling & 88.14 & 12 & 50 & depth=all\\
Random Forest & PCA with scaling & 87.69 & 34 & 50 & depth=all\\
Random Forest & SVD without scaling & 89.46 & 37 & 124 & depth=6\\ \hline
\end{tabular}

There were two reason to try SVD and PCA in the preprocessing of the data. The first one was to project the data in a more suitable base such that the random forest would require less trees to classify the data. The second was to get rid of unnecessary or redundant features. The depth of the dimension reduction is given by the *depth* parameter. For PCA the best results were obtained by including all the features while for SVD we observed that the optimal depth was for the first 6 components, reducing  the computation time significantly. Those results are somehow reasonable since the random sample of features in the random forests are thought to reduce the effect of noisy features. In a last trial, not reported, we used the relative importance of the features from the output of the randomForest method to get rid of the less important features. Unfortunately, dimension reduction returned reduced accuracy in crossvalidation.

**Support Vector Machines**


We first tried using SVMs to classify one group against all others, but this method did not perform nearly as well as the kNN and randomForest methods. Given the enormous amount of computing time SVMs require, we explored other SVM models and packages in R and came across specifications that are adaptable to multiclass problems.

A brief note should be made about the methodology to find the ideal parameters in the SVM models considered. The initial approach was to estimate and perform diagnostics on the model trained on the entire training set. However, the computation time was over 4 hours in initial trials, so we adjusted our strategy and tuned the SVM parameters on only 20% of the training set provided. This approach has allowed a greater set of models to be tested while still maintaining the relative differences in performance across models.

All of the types of SVM models considered were run in two different versions, considering the transformation of the continuous variables. These were scaled according to subtracting the mean and dividing by standard deviation of the training sample. The rationale is that this would make the numerical optimisation routine more stable. Some experimentation with different kernels was done in the training of the SVM. The performing kernels were the RBF and the Laplace kernel;  the other kernels were severely underperforming in comparison.

One-against-all SVM
Initial set of models tested considered was the one against all SVM classification for each class. So given 7 classes, 7 different model were run. The performance of these is evaluated using both in-sample validation error and cross validation error.  The results are presented graphically below (for both scaled and unscaled features). Error measures were incredibly high and thus these models were discarded instantly.


Multi-class SVM
Since this is a multiclass classification problem, we decided to try a classic approach called the 'one-against-one' method where we train k(k-1)/2 binary classifiers. Here, k represents the number of classes (7 in our case). The appropriate class is determined using a voting scheme.  Once again, the RBF kernel was used for training, as experimentation with other kernels yielded inferior error rates (scaling the continuous variables provided miniscule improvement for this method).  

We found that increasing the regularization C parameter increased in-sample training accuracy significantly. The relationship of the C parameter and the error rates can be seen in the graph below. However, increasing the C parameter also greatly increases training time. This is because an increased C parameter assigns a higher penalty to misclassification and forces the SVM optimisation routine to search more look for a "harder" boundary. From the below graph we can see that marginal improvement from increasing C decreases greatly as C increases. We chose arbitrarily C=5000 as a reasonable parameter in accuracy/run time pay off. The final in sample misclassification error was 0.1072. The 10 fold cross validation however was much greater.

SVM models proved to be inefficient with regard to run time and accuracy when compared to other models we considered. Thus we discarded this class of models as suitable for the given problem. 


**Nearest Neighbours**

The nearest neighbor algorithm gave some promising results as a classifier.  We tried to optimize the k for the nearest neighbor algorithm on both the raw data and the data that centered and scaled the continuous features.  For both instances, the highest accuracy was found using k=1.  Suprisingly, scaling the continuous feautes to be much smaller and centered around 0 did not increased the predictability of this classifier. So we decided to use the raw data in future iterations of this algorithm.  Below is a chart displaying the accuracy of the kNN algorithms across k's for the scaled and unscaled data.


```{r, echo=FALSE}
knn_no_scale <- read.csv("knn_noscale_errors.csv", header=T)
knn_no_scale1<-aggregate(knn_no_scale$error, list(knn_no_scale$k),mean)
max<-c(rep(1,length(knn_no_scale1$x)))
knn_no_scale2<-data.frame(group="unscaled",k=knn_no_scale1$Group.1, accuracy=round(max-knn_no_scale1$x,3))

knn_scaled_errors <- read.csv("knn_scaled_errors.csv", header=T)
knn_scaled_errors1<-aggregate(knn_scaled_errors$error, list(knn_scaled_errors$k),mean)
knn_scaled_errors2<-data.frame(group="scaled", k=knn_scaled_errors1$Group.1, accuracy=round(max-knn_scaled_errors1$x,3))

knn_graph<-rbind(knn_no_scale2,knn_scaled_errors2)

ggplot(data=knn_graph,aes(x=k,y=accuracy, color=group)) + geom_point() + ggtitle("kNN Accuracy")+ theme_bw() +scale_x_continuous(breaks=c(1,3,5,7,9)) + theme(legend.title=element_blank())

```
Another technique we tried was using singular value decomposition (SVD) with the 1NN algorithm.  The SVD can be thought of as decomposing a matrix into a weighted, ordered sum of separable matrices.  The idea was that perhaps dimension reduction that focused on the most important orthogonal features would give us a better measure of distance between points.  While this method did perform well; it did not improve accuracy much more than 1NN. Below is a chart displaying the optimal number of SVD components to use with the 1NN algorithm.  The optimal number of components to use (the one with lowest error) was 16, but as you can see from the chart, accuracy does not increase much after including 10 components (which is still quite a large reduction in dimension).

```{r}

knn_svd <- read.csv("knn_svd_errors.csv", header=T)
knn_svd1<-aggregate(knn_svd$error, list(knn_svd$features),mean)
max<-c(rep(1,length(knn_svd1$x)))
knn_svd<-data.frame(no_components=knn_svd1$Group.1, accuracy=round(max-knn_svd1$x,3))

ggplot(data=knn_svd,aes(x=no_components,y=accuracy)) + geom_point() + ggtitle("SVD Feature Accuracy with 1Knn")+ theme_bw() + theme(legend.title=element_blank()) + 
  xlab("Number of Compnents")
```

**Best Classifier: Nearest Neighbours with weighted binary data**

In order to improve the accuracy we realized that the wild areas and soil types were underweighted in the calculation of the distance. This is because the scales of the features are substantially different. For example, the elevation ranges from 2000 to 6000 while the soil types and wild areas are binary, i. e. they can take values 0 or 1. The binary differences lead to an insignificant increase in the distance when kNN is comparing examples with and without a given binary feature. So the first thing we tryed is to scale the continuous variables. The results are .... (ADD DOCUMENTATION ON SCALED KNN)

To solve this issue we came back to the visual inspection of the data. In the calculation of the distance we want to keep the relevance of the continuous features since some of them are reasonably well separated (e.g. the elevation) and increase the relevance of the binary features. To do that we multiplyed the binary features by the same weight while keeping the continuous variables with the default scale. The optimal parameters we got were $weight=91$ and $k=1$. The figure below shows the optimization of the weights for $k=1$.

```{r}
kNN_OptimalWeight <- read.csv("1NN_OptimalWeight.csv")
names(kNN_OptimalWeight) <- c("Misclass", "Weight")
ggplot(data=kNN_OptimalWeight, 
       aes(x=Weight,y=(1-Misclass),size=2)) + 
  geom_point() +
  ylab(c("Accuracy")) +
  theme_bw() + 
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        text = element_text(size=14),
        legend.position="none")
```

We also checked systematically that the best k was 1 for the different weights. The optimal results for other weighting and scaling strategies we checked are summarized in the following table

\begin{tabular}{|l|l||l|l||l|l|}\hline
Method & Preprocessing & Accuracy(\%) & k & Weight & Other Params\\ \hline
kNN & raw data &   & 1 & - & \\
kNN & Scaled continuous &  & 1 & N(0,1) & \\
kNN & Weighted binaries & 91.88 & 1 & 91 & \\
kNN & SVD & & 1 &  & \\ \hline
\end{tabular}

**Conclusions**

We conclude that the best method we could find to classify the forest covertypes was the kNN with k=1 with a weight of 91 applied to the binary variables, so values were$\in \lbrace 0, 91 \rbrace$. This method is responsible for our best Kaggle submission: an accuracy rate of $92.52\%$  in the $10\%$ test set. Our approach relies on the fact that the binary variables are underweighted in the calculation of the euclidean distances used in kNN. Since the scaling of the continuous variables didn't give us improved results we decided to scale all the binary variables with a fixed weight. This method gives the best compromise between the relevance of the continuous features and the binary features.  It is possible with more time and exploration that weights could vary across for each individual binary feature, but this was not something we explored.

We discarded many methods in our search for the optimal classifier; many performed only slightly worse than our final classifier.  The PCA preprocessing was the most disappointing; it probably underperformed because it is designed to reduce dimensionality and capture as much variance as possible in very few variables.  While this method did manage to do that, the dimensions that it excluded probably contained enough accuracy to provide one or two extra points of accuracy in other methods.  SVMs also seemed to underperform for our classification problem.  Not only was the accuracy below that of other classifiers, this paticular method is much more computationally expensive than simpler and better performing methods such as kNN.